import os
# prevent jax from gobbling up all the gpu memory
os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"

import pickle
import numpy as np
import bmws.data
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm

from bmws.infer import Selection, gibbs, sample_paths
from bmws.util import tree_stack
import bmws.data

import jax
import jax.numpy as jnp

N_E = 1e4
RUNTIME = 180

def plot_trajectories(pop, outfile, slns, paths, data):
    pop_cols = {
        "EAS": ["#2F5233", "#B1D8B7", "#76B947"],
        "EUR": ["#BBD2FD", "#63D5F9", "#2A67D6"],
        "SAM": ["#B03437", "#5885AF"],
        "SAS": ["#AE307F", "#6E2ED9"],
        "AFR": ["#FF8247", "#CD853F", "#E8AD17"],
    }
    pop_cols["TEST"] = pop_cols["EAS"]
    cols = pop_cols[pop]
    K = len(cols)

    t = np.arange(data.T)

    s = np.array([si(t) for si in slns])

    M = len(s)
    B = int(len(s) * .1)

    low, mid, high = np.quantile(s[B:][::50], [0.05, 0.5, 0.95], axis=0)  # [T, K]

    fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(10, 3))
    for k in range(K):
        # axs[0].plot(s[:, k], color=cols[k], alpha=1)
        axs[0].plot(mid[:, k], color=cols[k], alpha=1)
        axs[0].fill_between(
            t, low[:, k], high[:, k], alpha=0.2, color=cols[k]
        )

    paths = np.array(paths)[B:][::50]
    paths = paths[:, ::-1] / 2e4
    high = np.max(paths, axis=0)
    low = np.min(paths, axis=0)
    for k in range(K):
        axs[1].plot(np.mean(paths, axis=0)[:, k], color=cols[k])
        axs[1].fill_between(
            t, low[:, k], high[:, k], alpha=0.2, color=cols[k]
        )

    mp = bmws.data.bootstrap_paths(data, 1000, 42)

    a = [int(y) for x, y in zip(data.obs, data.t) if x[0] > 0]
    b = [x[1] for x, y, theta in zip(data.obs, data.t, data.theta) if x[0] > 0]
    sns.regplot(
        x=a,
        y=b,
        logistic=True,
        scatter=True,
        ax=axs[2],
        line_kws={"color": "black", "alpha": 0.5},
    )
    axs[0].set(xlabel="Generations before present", ylabel="Selection coefficient")
    axs[1].set(xlabel="Generations before present", ylabel="Allele frequency")
    axs[2].set(xlabel="Generations before present", ylabel="Observations")
    # fig.suptitle(pop+": "+snp, y=1.1)
    plt.tight_layout()
    fig.savefig(outfile)


def read_data(pop):
    pop = pop.lower()
    admixture_proportions = pd.read_csv("data/" + pop + "_sample_info.txt", sep="\t")
    admixture_proportions["generation"] = [
        int(x) for x in round(admixture_proportions["Date"] / 30)
    ]
    admixture_proportions = admixture_proportions[
        (admixture_proportions["Date"] <= 10000)
    ]

    # merge allele counts
    counts = pd.read_csv("data/" + pop + "_snp_acs.raw", sep="\t")
    snps = list(counts.columns)[6:]
    data = pd.merge(admixture_proportions, counts, on="IID")

    # Parameters for data matrices
    T = max(data["generation"]) + 1
    N = max(data["generation"].value_counts().values)
    K = admixture_proportions.shape[1] - 7
    datasets = []
    for snp in snps:
        records = []
        for gen, count in data["generation"].value_counts().items():
            this_data = data[data["generation"] == gen]
            M = this_data.shape[0]
            for i in range(M):
                if not this_data[snp].isna().iloc[i]:
                    rec = {"t": gen}
                    rec["obs"] = (1, int(this_data[snp].values[i] / 2))
                    rec["theta"] = [
                        this_data["k" + str(k + 1)].iloc[i] for k in range(K - 1)
                    ]
                    rec["theta"].append(1 - sum(rec["theta"]))
                    records.append(rec)

        datasets.append(bmws.data.Dataset.from_records(records))

    return datasets, snps

def run_analysis(data, sln0=None, alpha=1e4, em_iterations=50, M=10_000):
    if sln0 is None:
        sln0 = Selection.default(T=data.T, K=data.K)
    mp = bmws.data.regression_paths(data, M)
    return gibbs(sln0=sln0, data=data, alpha=alpha, mean_paths=mp, niter=em_iterations, M=M, seed=42, N_E=N_E)

def resample_individuals(data, Ne, rng, em_iterations, alpha, M):
    # Now resample observations
    t = data.t
    theta = data.theta
    obs = data.obs
    records = []
    Tmax = int(max(t))
    present_obs = (data.t == 0).nonzero()[0]
    nonzero_anc_obs = np.logical_and(data.obs[:, 0] > 0, data.t != 0).nonzero()[0]
    for k in range(len(present_obs)):  # keep present-day individuals.
        pik = present_obs[k]
        rec = {"t": data.t[pik]}
        rec["obs"] = (int(obs[pik][0]), int(obs[pik][1]))
        rec["theta"] = data.theta[pik]
        records.append(rec)
    pik = 0  # include earlist timepoint to maintain size
    rec = {"t": data.t[pik]}
    rec["obs"] = (int(obs[pik][0]), int(obs[pik][1]))
    rec["theta"] = data.theta[pik]
    records.append(rec)
    for k in range(len(nonzero_anc_obs)):  # resample ancient individuals
        pik = rng.choice(nonzero_anc_obs)
        rec = {"t": data.t[pik]}
        rec["obs"] = (int(obs[pik][0]), int(obs[pik][1]))
        rec["theta"] = data.theta[pik]
        records.append(rec)
    dataset_samples = bmws.data.Dataset.from_records(records)
    # run inference
    slns, paths = run_analysis(dataset_samples, alpha=alpha, em_iterations=em_iterations, M=M)
    return sln_star, dataset_samples


def dump_file(obj, fn):
    with open(fn, "wb") as f:
        pickle.dump(obj, file=f)


def load_file(fn):
    with open(fn, "rb") as f:
        return pickle.load(f)


wildcard_constraints:
    population=r"[A-Z]+",
    seed=r"\d+",
    snp=r"\w+",


rule read_data_test:
    output:
        "results/TEST/data.pkl",
    run:
        ds = bmws.data.Dataset.from_records(
            [
                dict(t=20, theta=np.array([0.1, 0.3, 0.6]), obs=(1, 1)),
                dict(t=18, theta=np.array([1.0, 0.0, 0.0]), obs=(1, 0)),
                dict(t=10, theta=np.array([0.1, 0.4, 0.5]), obs=(2, 1)),
                dict(t=10, theta=np.array([0.1, 0.0, 0.9]), obs=(5, 2)),
                dict(t=10, theta=np.array([0.5, 0.0, 0.5]), obs=(1, 1)),
                dict(t=2, theta=np.array([0.2, 0.5, 0.3]), obs=(1, 1)),
                dict(t=2, theta=np.array([0.5, 0.2, 0.3]), obs=(1, 0)),
                dict(t=1, theta=np.array([0.5, 0.2, 0.3]), obs=(1, 1)),
                dict(t=0, theta=np.array([0.0, 1.0, 0.0]), obs=(3, 2)),
                dict(t=0, theta=np.array([0.2, 0.2, 0.6]), obs=(4, 1)),
            ]
        )
        snps = ["test"]
        dump_file(([ds], snps), output[0])


rule read_data:
    output:
        "results/{population}/data.pkl",
    run:
        jax.config.update("jax_platforms", "cpu")
        res = read_data(wildcards.population)
        dump_file(res, output[0])


def _params(wc):
    alpha = 10 ** float(wc.log_alpha)
    em_iterations = int(wc.em_iterations)
    M = int(wc.M)
    return alpha, em_iterations, M

rule prior:
    input:
        "results/{population}/data.pkl",
    output:
        "results/{population}/{snp}/prior_{M}.pkl"
    run:
        jax.config.update("jax_platforms", "cpu")
        datasets, snps = load_file(input[0])
        wc = wildcards
        try:
            data = datasets[snps.index(wc.snp)]
        except ValueError:
            raise ValueError(f"Cannot find SNP {wc.snp}. SNPS are {snps}")
        M = int(wc.M)
        mp = bmws.data.regression_paths(data, M)
        dump_file(mp, output[0])


rule analyze:
    input:
        "results/{population}/data.pkl",
    output:
        "results/{population}/{snp}/a{log_alpha}/em{em_iterations}/M{M}/analyze.pkl"
    resources:
        gpus=1,
        runtime=RUNTIME,
        slurm_partition="spgpu,gpu",
        slurm_extra="--gpus 1",
        mem="16G",
    threads: 2
    run:
        datasets, snps = load_file(input[0])
        wc = wildcards
        M = int(wc.M)
        try:
            data = datasets[snps.index(wc.snp)]
        except ValueError:
            raise ValueError(f"Cannot find SNP {wc.snp}. SNPS are {snps}")
        Ne = 1e4
        alpha, em_iterations, M = _params(wildcards)
        sln0 = Selection.default(T=data.T, K=data.K)
        res = gibbs(sln0, data, alpha, em_iterations, M, seed=42)
        dump_file(res, output[0])


rule resample:
    input:
        "results/{population}/data.pkl",
        "results/{population}/{snp}/a{log_alpha}/em{em_iterations}/M{M}/analyze.pkl"
    output:
        "results/{population}/{snp}/a{log_alpha}/em{em_iterations}/M{M}/resample_{seed}.pkl",
    threads: 8
    resources:
        gpus=1,
        runtime=RUNTIME,
        slurm_partition="spgpu,gpu",
        slurm_extra="--gpus 1",
        mem="16G",
    run:
        datasets, snps = load_file(input[0])
        (sln, prior), aux = load_file(input[1])
        data = datasets[snps.index(wildcards.snp)]
        alpha, em_iterations, M = _params(wildcards)
        seed = int(wildcards.seed)
        rng = np.random.default_rng(seed)
        res = resample_individuals(data, N_E, rng, em_iterations, alpha, M)
        dump_file(res, output[0])


rule collect_resamples:
    localrule: True
    input:
        expand("results/{{stuff}}/resample_{seed}.pkl", seed=range(10)),
    output:
        "results/{stuff}/resamples.pkl",
    run:
        jax.config.update("jax_platforms", "cpu")
        ss = [s for s, _ in map(load_file, input)]
        S = tree_stack(ss)
        dump_file(S, output[0])


rule plot:
    input:
        "results/{population}/data.pkl",
        "results/{population}/{snp}/{stuff}/analyze.pkl",
    output:
        config["figure_path"] + "/{stuff}/{population}_{snp}.pdf",
    localrule: True
    run:
        jax.config.update("jax_platforms", "cpu")
        datasets, snps = load_file(input[0])
        data = datasets[snps.index(wildcards.snp)]
        slns, paths = load_file(input[1])[:2]
        plot_trajectories(wildcards.population, output[0], slns, paths, data)


rule altplot:
    input:
        config["figure_path"] + "/{stuff}/{population}_{snp}.pdf",
    output:
        "results/{population}/{snp}/{stuff}/plot.pdf"
    shell:
        "cp {input} {output}"

ALL_SNPS = {
    "EAS": ["snp_4_100142302_AC", "rs174548_CG"],
    "EUR": ["rs174548_CG", "rs4988235_AG", "rs12913832_GA", "rs16891982_GC"],
    "SAM": ["rs5760093_AG"]
}

rule all:
    input:
        [
            config["figure_path"] + f"/a{a}/em2000/M1000/{population}_{snp}.pdf"
            for population in ALL_SNPS
            for snp in ALL_SNPS[population]
            for a in [2.5]
        ]

